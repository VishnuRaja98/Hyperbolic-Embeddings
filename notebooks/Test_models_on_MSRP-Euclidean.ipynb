{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "491a3530-0cb4-47e4-845c-72fdc690d68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cronus_data/vraja/dysarthria/lib/python3.11/site-packages/transformers/utils/hub.py:105: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "!!!!!!total problem rows =  0\n",
      "Loaded 1725 sentence pairs\n",
      "Evaluating model: best_model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1229612/1508235607.py:358: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n",
      "Evaluating: 100%|███████████████████████████████████████████████████████████████████████| 54/54 [00:03<00:00, 16.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: best_model\n",
      "ROC AUC: 0.7261\n",
      "PR AUC: 0.8373\n",
      "Best F1 Score: 0.8082\n",
      "Optimal Threshold: 0.4848\n",
      "Precision at optimal threshold: 0.6908\n",
      "Recall at optimal threshold: 0.9738\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import random_split\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from scipy.stats import spearmanr\n",
    "from tqdm import tqdm \n",
    "import geoopt\n",
    "from datetime import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from scipy.special import expit  # For sigmoid function\n",
    "\n",
    "# Assuming you've already defined your model and poincare_distance function as in the original code\n",
    "# Let's redefine them here to make this script standalone\n",
    "\n",
    "class SentenceEncoder(nn.Module):\n",
    "    def __init__(self, model_name='sentence-transformers/all-MiniLM-L6-v2'):\n",
    "        super(SentenceEncoder, self).__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Use CLS token representation\n",
    "        cls_embedding = output.last_hidden_state[:, 0]\n",
    "        cls_embedding = F.normalize(cls_embedding, p=2, dim=1)  # normalize for cosine similarity\n",
    "        return cls_embedding\n",
    "\n",
    "\n",
    "# Dataset class for evaluating on pairs with binary labels\n",
    "class BinaryLabelDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, max_length=128):\n",
    "        self.data = self.read_file(file_path)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        print(f\"Loaded {len(self.data)} sentence pairs\")\n",
    "\n",
    "    def read_file(self, file_path):\n",
    "        data = []\n",
    "        problem_rows = 0\n",
    "    \n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            csv_reader = csv.reader(file, delimiter='\\t', quotechar=None)\n",
    "            headers = next(csv_reader, None)  # Read and skip the header row\n",
    "    \n",
    "            for row in csv_reader:\n",
    "                if len(row) >= 5:\n",
    "                    sentence1, sentence2, label_str = row[3], row[4], row[0] \n",
    "                    try:\n",
    "                        # Ensure label is either 0 or 1 (binary)\n",
    "                        label = int(float(label_str))  # Support for both integer and float formats\n",
    "                        if label not in [0, 1]:\n",
    "                            # Normalize any other value to binary (0 or 1)\n",
    "                            # Typically, values > 0 could be considered paraphrases\n",
    "                            label = 1 if label > 0 else 0\n",
    "                        data.append((sentence1.strip(), sentence2.strip(), label))\n",
    "                    except:\n",
    "                        continue\n",
    "                else:\n",
    "                    problem_rows += 1\n",
    "    \n",
    "        print(\"!!!!!!total problem rows = \", problem_rows)\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence1, sentence2, label = self.data[idx]\n",
    "        \n",
    "        # Tokenize both sentences\n",
    "        sent1_input = self.tokenizer(\n",
    "            sentence1,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "            return_token_type_ids=False\n",
    "        )\n",
    "        \n",
    "        sent2_input = self.tokenizer(\n",
    "            sentence2,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "            return_token_type_ids=False\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'sent1_input': {k: v.squeeze(0) for k, v in sent1_input.items()},\n",
    "            'sent2_input': {k: v.squeeze(0) for k, v in sent2_input.items()},\n",
    "            'label': torch.tensor(label, dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_fn_eval(batch):\n",
    "    sent1_inputs = {\n",
    "        k: torch.stack([item['sent1_input'][k] for item in batch])\n",
    "        for k in batch[0]['sent1_input']\n",
    "    }\n",
    "    \n",
    "    sent2_inputs = {\n",
    "        k: torch.stack([item['sent2_input'][k] for item in batch])\n",
    "        for k in batch[0]['sent2_input']\n",
    "    }\n",
    "    \n",
    "    labels = torch.stack([item['label'] for item in batch])\n",
    "    \n",
    "    return {\n",
    "        'sent1_input': sent1_inputs,\n",
    "        'sent2_input': sent2_inputs,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_similarity(embed1, embed2):\n",
    "    \"\"\"Compute cosine similarity between embeddings.\"\"\"\n",
    "    # Since embeddings are already L2 normalized (in the model's forward pass),\n",
    "    # the dot product equals cosine similarity\n",
    "    return torch.sum(embed1 * embed2, dim=1)\n",
    "\n",
    "\n",
    "def find_optimal_threshold(labels, similarities):\n",
    "    \"\"\"Find optimal threshold for binary classification.\n",
    "    \n",
    "    Args:\n",
    "        labels: Ground truth labels\n",
    "        similarities: Similarity scores\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (optimal_threshold, max_f1)\n",
    "    \"\"\"\n",
    "    # Calculate F1 score for different thresholds\n",
    "    f1_scores = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    thresholds = np.linspace(0, 1, 100)\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        predictions = (similarities >= threshold).astype(int)\n",
    "        \n",
    "        # Calculate precision and recall\n",
    "        true_positives = np.sum((predictions == 1) & (labels == 1))\n",
    "        false_positives = np.sum((predictions == 1) & (labels == 0))\n",
    "        false_negatives = np.sum((predictions == 0) & (labels == 1))\n",
    "        \n",
    "        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        \n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        f1_scores.append(f1)\n",
    "    \n",
    "    # Find threshold with maximum F1 score\n",
    "    optimal_idx = np.argmax(f1_scores)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    max_f1 = f1_scores[optimal_idx]\n",
    "    best_precision = precisions[optimal_idx]\n",
    "    best_recall = recalls[optimal_idx]\n",
    "    \n",
    "    return optimal_threshold, max_f1, best_precision, best_recall\n",
    "\n",
    "\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    \"\"\"Evaluate model on binary classification task.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained sentence encoder model\n",
    "        data_loader: DataLoader for evaluation data\n",
    "        device: Computation device\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with evaluation metrics\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_similarities = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            # Move inputs to device\n",
    "            sent1_input = {k: v.to(device) for k, v in batch['sent1_input'].items()}\n",
    "            sent2_input = {k: v.to(device) for k, v in batch['sent2_input'].items()}\n",
    "            labels = batch['labels'].cpu().numpy()\n",
    "            \n",
    "            # Get embeddings\n",
    "            sent1_embed = model(**sent1_input)\n",
    "            sent2_embed = model(**sent2_input)\n",
    "            \n",
    "            # Calculate cosine similarities\n",
    "            similarities = compute_similarity(sent1_embed, sent2_embed).cpu().numpy()\n",
    "            \n",
    "            # Store for later computation\n",
    "            all_similarities.extend(similarities)\n",
    "            all_labels.extend(labels)\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    all_similarities = np.array(all_similarities)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    # Find optimal threshold and F1 score\n",
    "    threshold, max_f1, best_precision, best_recall = find_optimal_threshold(all_labels, all_similarities)\n",
    "    \n",
    "    # Calculate ROC curve\n",
    "    fpr, tpr, _ = roc_curve(all_labels, all_similarities)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Calculate PR curve\n",
    "    precision, recall, _ = precision_recall_curve(all_labels, all_similarities)\n",
    "    pr_auc = average_precision_score(all_labels, all_similarities)\n",
    "    \n",
    "    return {\n",
    "        'roc_auc': roc_auc,\n",
    "        'pr_auc': pr_auc,\n",
    "        'f1_score': max_f1,\n",
    "        'optimal_threshold': threshold,\n",
    "        'best_precision': best_precision,\n",
    "        'best_recall': best_recall,\n",
    "        'fpr': fpr,\n",
    "        'tpr': tpr,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'similarities': all_similarities,\n",
    "        'labels': all_labels\n",
    "    }\n",
    "\n",
    "\n",
    "def plot_curves(results, model_name, save_dir=\"./plots\"):\n",
    "    \"\"\"Plot ROC and PR curves for the evaluation results.\n",
    "    \n",
    "    Args:\n",
    "        results: Dictionary with evaluation metrics\n",
    "        model_name: Name of the model for plot titles\n",
    "        save_dir: Directory to save plots\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # ROC curve\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.plot(results['fpr'], results['tpr'], lw=2, label=f'ROC curve (AUC = {results[\"roc_auc\"]:.3f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curve - {model_name}')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig(f\"{save_dir}/{model_name}_roc.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # PR curve\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.plot(results['recall'], results['precision'], lw=2, label=f'PR curve (AUC = {results[\"pr_auc\"]:.3f})')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title(f'Precision-Recall Curve - {model_name}')\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    plt.savefig(f\"{save_dir}/{model_name}_pr.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # Similarity distribution\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    positive_sim = results['similarities'][results['labels'] == 1]\n",
    "    negative_sim = results['similarities'][results['labels'] == 0]\n",
    "    \n",
    "    plt.hist(positive_sim, bins=50, alpha=0.5, label='Positive pairs', density=True)\n",
    "    plt.hist(negative_sim, bins=50, alpha=0.5, label='Negative pairs', density=True)\n",
    "    plt.axvline(x=results['optimal_threshold'], color='r', linestyle='--', \n",
    "                label=f'Optimal threshold = {results[\"optimal_threshold\"]:.3f}')\n",
    "    plt.xlabel('Cosine Similarity')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title(f'Similarity Distribution - {model_name} (F1 = {results[\"f1_score\"]:.3f})')\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{save_dir}/{model_name}_sim_dist.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # Threshold vs F1 Score\n",
    "    thresholds = np.linspace(0, 1, 100)\n",
    "    f1_scores = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        predictions = (results['similarities'] >= threshold).astype(int)\n",
    "        f1 = f1_score(results['labels'], predictions)\n",
    "        f1_scores.append(f1)\n",
    "        \n",
    "        # Calculate precision and recall\n",
    "        true_positives = np.sum((predictions == 1) & (results['labels'] == 1))\n",
    "        false_positives = np.sum((predictions == 1) & (results['labels'] == 0))\n",
    "        false_negatives = np.sum((predictions == 0) & (results['labels'] == 1))\n",
    "        \n",
    "        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(thresholds, f1_scores, label='F1 Score')\n",
    "    plt.plot(thresholds, precisions, label='Precision')\n",
    "    plt.plot(thresholds, recalls, label='Recall')\n",
    "    plt.axvline(x=results['optimal_threshold'], color='r', linestyle='--', \n",
    "                label=f'Optimal threshold = {results[\"optimal_threshold\"]:.3f}')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title(f'Metrics vs Threshold - {model_name}')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig(f\"{save_dir}/{model_name}_threshold_metrics.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "    \n",
    "    # Path to your binary labeled dataset\n",
    "    binary_dataset_path = 'MSRParaphraseCorpus/msr_paraphrase_test.txt'  # Replace with your dataset path\n",
    "    \n",
    "    # Load the binary dataset\n",
    "    binary_dataset = BinaryLabelDataset(binary_dataset_path, tokenizer)\n",
    "    \n",
    "    # Create DataLoader\n",
    "    binary_loader = DataLoader(\n",
    "        binary_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn_eval\n",
    "    )\n",
    "\n",
    "    db_name = \"msrp\"\n",
    "    model_name = \"best_model\"\n",
    "    model_path = f'{model_name}.pt'\n",
    "        \n",
    "    print(f\"Evaluating model: {model_path}\")\n",
    "    \n",
    "    model = SentenceEncoder().to(device)\n",
    "        \n",
    "    # Load weights\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "\n",
    "    # Evaluate the model\n",
    "    results = evaluate_model(model, binary_loader, device)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"ROC AUC: {results['roc_auc']:.4f}\")\n",
    "    print(f\"PR AUC: {results['pr_auc']:.4f}\")\n",
    "    print(f\"Best F1 Score: {results['f1_score']:.4f}\")\n",
    "    print(f\"Optimal Threshold: {results['optimal_threshold']:.4f}\")\n",
    "    print(f\"Precision at optimal threshold: {results['best_precision']:.4f}\")\n",
    "    print(f\"Recall at optimal threshold: {results['best_recall']:.4f}\")\n",
    "\n",
    "    # Plot curves\n",
    "    plot_curves(results,db_name, model_name)\n",
    "\n",
    "    # Save results\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "    with open(f\"results/{db_name}_{model_name}_eval.json\", 'w') as f:\n",
    "        json_results = {\n",
    "            'model': model_name,\n",
    "            'roc_auc': float(results['roc_auc']),\n",
    "            'pr_auc': float(results['pr_auc']),\n",
    "            'f1_score': float(results['f1_score']),\n",
    "            'optimal_threshold': float(results['optimal_threshold']),\n",
    "            'precision': float(results['best_precision']),\n",
    "            'recall': float(results['best_recall'])\n",
    "        }\n",
    "        json.dump(json_results, f, indent=2)\n",
    "\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Create directories if they don't exist\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "os.makedirs(\"plots\", exist_ok=True)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ee5124-3e70-4612-bd25-fb0e27e8af95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87821461-b8aa-4a3f-b136-891f3793f81f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
